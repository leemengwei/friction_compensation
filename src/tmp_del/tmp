import time
from IPython import embed
import numpy as np
import os,sys,time

def compute_loss(Y, X, params, args):
    if args.mode == 'linear':
        c0, c1, c2, c3, c4 = params[0], params[1], params[2], params[3], params[4]
        friction_predicted = linear_friction_model(X, c0, c1, c2, c3, c4)
        partials = linear_friction_model_derivatives(X)
    else:
        c0, v_brk, F_brk, F_C, c1, c2, c3, c4 = params[0], params[1], params[2], params[3], params[4], params[5], params[6], params[7]
        friction_predicted = nonlinear_friction_model(X, c0, v_brk, F_brk, F_C, c1, c2, c3, c4)
        partials = nonlinear_friction_model_derivatives(X, v_brk, F_C, F_brk)
    loss = friction_predicted - Y
    J = np.sum(loss**2)/(2*X.shape[1]) 
    return loss, J, friction_predicted, params, partials


def linear_friction_model(X, c0, c1, c2, c3, c4):
    f = X[0]*c0 + X[1]*c1 + X[2]*c2 + X[3]*c3 + X[4]*c4
    return f

def linear_friction_model_derivatives(X):
    partial_f_c0 = X[0]
    partial_f_c1 = X[1]
    partial_f_c2 = X[2]
    partial_f_c3 = X[3]
    partial_f_c4 = X[4]
    partials = np.vstack((partial_f_c0, partial_f_c1, partial_f_c2, partial_f_c3, partial_f_c4))
    return partials 


def nonlinear_friction_model(X, c0, v_brk, F_brk, F_C, c1, c2, c3, c4):
    #F_brk = F_C
    f = X[0]*c0 + 0*(np.sqrt(2*np.e)*(F_brk-F_C)*np.exp(-(X[1]/(v_brk*np.sqrt(2)))**2)*(X[1]/(v_brk*np.sqrt(2))) + F_C*np.tanh(X[1]/(v_brk/10))) + X[1]*c1 + X[2]*c2 + X[3]*c3 + X[4]*c4
    return f

def nonlinear_friction_model_derivatives(X, v_brk, F_C, F_brk):
    partial_f_c0 = X[0]
    partial_f_v_brk = -10*F_C*X[1]*(1 - np.tanh(10*X[1]/v_brk)**2)/v_brk**2 + (1/np.sqrt(2))*X[1]**3*((-np.sqrt(2*np.e))*F_C + (np.sqrt(2*np.e))*F_brk)*np.exp(-0.5*X[1]**2/v_brk**2)/v_brk**4 - (1/np.sqrt(2))*X[1]*((-np.sqrt(2*np.e))*F_C + (np.sqrt(2*np.e))*F_brk)*np.exp(-0.5*X[1]**2/v_brk**2)/v_brk**2
    partial_f_F_brk = (np.sqrt(2*np.e)/np.sqrt(2))*X[1]*np.exp(-0.5*X[1]**2/v_brk**2)/v_brk
    partial_f_F_C = -(np.sqrt(2*np.e)/np.sqrt(2))*X[1]*np.exp(-0.5*X[1]**2/v_brk**2)/v_brk + np.tanh(10*X[1]/v_brk)
    partial_f_c1 = X[1]
    partial_f_c2 = X[2]
    partial_f_c3 = X[3]
    partial_f_c4 = X[4]
    partials = np.vstack((partial_f_c0, partial_f_v_brk, partial_f_F_brk, partial_f_F_C, partial_f_c1, partial_f_c2, partial_f_c3, partial_f_c4))
    return partials 


import glob
import sav_smooth
import pandas as pd
import time
from IPython import embed
import numpy as np
import os,sys,time

def get_standarlized_data():
    files = glob.glob("../data/*.prb-log")
    datas = pd.DataFrame()
    quick_path = "../data/tmp_del/all_data.csv"
    if os.path.exists(quick_path):
        print("Read from quick data: %s"%quick_path)
        datas = pd.read_csv("%s"%quick_path, index_col=0)
        return datas
    for _file_ in files:
        data = pd.read_csv(_file_, sep=' ')
        data = data.drop(data.shape[0]-1).astype(float)
        data = data.drop('line', axis=1)
        data = data_retrieve(data)
        times = data['ms']
        friction_value_axis_3 = data['servo_feedback_torque_3']-data['axc_torque_ffw_gravity_3']    #TODO: this is friction on axis 4
        data['need_to_compensate'] = friction_value_axis_3
        data['Temp'] = 0
        datas = pd.concat((datas, data))
        print(_file_, len(datas))
    datas.to_csv("../data/tmp_del/all_data.csv")
    print("Dumped to quick data: %s"%quick_path)
    return datas


class normalizer(object):
    def __init__(self, X, Y):
        self._X_mean_never_touch_ =  np.tile(np.array([ 1.00000000e+00, -1.22205200e+00, -3.62142402e-03,  0.00000000e+00, 4.36526466e+00]), (X.shape[1], 1)).T
        self._X_std_never_touch_ = np.tile(np.array([1.00000000e-07, 1.03640789e+02, 4.98661209e-02, 1.00000000e-07, 1.31607409e+02]), (X.shape[1], 1)).T
        self._Y_mean_never_touch_ = -0.0014823869995695225
        self._Y_std_never_touch_ = 0.10306605601654062
    def normalize(self, X, Y):
        return (X-self._X_mean_never_touch_)/self._X_std_never_touch_, (Y-self._Y_mean_never_touch_)/self._Y_std_never_touch_
    def denormlize(self, Y):
        return Y*self._Y_std_never_touch_+self._Y_mean_never_touch_


def data_retrieve(data):
    def _data_drop_acc(data):
        smoothed_v = sav_smooth.savitzky_golay(data['servo_feedback_speed_3'].values, 101, 1)
        deltas = smoothed_v[1:] - smoothed_v[:-1]
        _threshold = 0.1  #threshold for v average
        #plt.plot(data['ms'], data['servo_feedback_speed_3'])
        #plt.scatter(data.iloc[list(np.where(np.abs(deltas)>_threshold)[0])].ms, data.iloc[list(np.where(np.abs(deltas)>_threshold)[0])].servo_feedback_speed_3, 3, 'r')
        data = data.drop(list(np.where(np.abs(deltas)>_threshold)[0]))
        #plt.plot(data['ms'], data['servo_feedback_speed_3'])
        #plt.scatter(data.ms, data.servo_feedback_speed_3, 3, 'r')
        return data
    def _data_stay(data):
        compensator = 0.9   #to make sure all data at fastest routine (yet turbulent) are kept.
        chance_to_stay = np.abs(data['servo_feedback_speed_3'])/(np.abs(data['servo_feedback_speed_3']).max()*compensator)
        which_to_stay = (np.random.random(len(chance_to_stay)) < chance_to_stay)
        data = data[which_to_stay]
        return data
    #Get rid of accelerating data
    data_dropped = _data_drop_acc(data)
    #And re-scale by position (achived by chance)
    data_stayed = _data_stay(data_dropped)
    return data_stayed


#coding:utf-8
import time
from IPython import embed
import numpy as np
import os,sys,time

import matplotlib
#matplotlib.use('TKAgg')
import matplotlib.pyplot as plt
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D
from scipy.optimize import curve_fit
import scipy
import argparse
from tqdm import tqdm

#my modules:
import data_stuff
import classical_model
import plot_utils
import NN_model

import pickle as pkl
import PIL
import torch
import torch.utils.data as Data
import torch.optim as optim
import torch.nn.functional as F
torch.manual_seed(44)

plt.ion()
DEBUG = False
DEBUG = True
epsilon = 1e-10

def evaluate_error_rate(outputs, data, normer, showup=False):
    outputs = normer.denormlize(outputs)
    value_threshold = 0.01
    low_value_region = np.where(np.abs(data['servo_feedback_torque_3'])<value_threshold)
    high_value_region = np.where(np.abs(data['servo_feedback_torque_3'])>=value_threshold)
    if args.VISUALIZATION and showup:
        plt.hist(data['servo_feedback_torque_3'].values[low_value_region], bins=50)
        plt.hist(data['servo_feedback_torque_3'].values[high_value_region], bins=1000)
        plt.title("Split of feedback (as denominator)")
        plt.show()
    error_rate_low = np.abs((data.iloc[low_value_region]['axc_torque_ffw_gravity_3'] + outputs[low_value_region] - data.iloc[low_value_region]['servo_feedback_torque_3'])/(data.iloc[low_value_region]['servo_feedback_torque_3']+epsilon)).mean()*100
    error_rate_high = np.abs((data.iloc[high_value_region]['axc_torque_ffw_gravity_3'] + outputs[high_value_region] - data.iloc[high_value_region]['servo_feedback_torque_3'])/(data.iloc[high_value_region]['servo_feedback_torque_3']+epsilon)).mean()*100
    return (error_rate_low, error_rate_high)


def train(args, model, device, train_loader, optimizer, epoch):
    model = model.to(device)
    model.train()
    LOSS = 0
    pbar = tqdm(train_loader)
    for batch_idx, (data, target) in enumerate(pbar):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model((data))
        loss = F.mse_loss(output, target)
        loss.backward()
        LOSS += F.mse_loss(output, target, reduction='sum').item() # pile up all loss
        optimizer.step()
        if (batch_idx % args.log_interval == 0 or batch_idx == len(train_loader)-1)and(batch_idx!=0):
            #pbar.set_description('Train Epoch: {} [{}/{} ({:.0f}%)]. Loss: {:.8f}'.format(epoch, batch_idx*len(data), len(train_loader.dataset), 100.*batch_idx/len(train_loader), loss.item()))
            pass
    train_loss_mean = LOSS/len(train_loader.dataset)
    return train_loss_mean, output, target

def validate(args, model, device, validate_loader):
    model = model.to(device)
    model.eval()
    LOSS = 0
    with torch.no_grad():
        pbar = tqdm(validate_loader)
        for idx, (data, target) in enumerate(pbar):
            data, target = data.to(device), target.to(device)
            output = model((data))
            LOSS += F.mse_loss(output, target, reduction='sum').item() # pile up all loss
            #pbar.set_description('Validate: [{}/{} ({:.0f}%)]'.format(idx*len(data), len(validate_loader.dataset), 100.*idx/len(validate_loader)))
    validate_loss_mean = LOSS/len(validate_loader.dataset)
    return validate_loss_mean, output, target


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Friction.')
    parser.add_argument('--mode', '-M', default='linear')
    parser.add_argument('--learning_rate', '-LR', type=float, default=0.01)
    parser.add_argument('--test_ratio', '-TR', type=float, default=0.2)
    parser.add_argument('--criterion', '-C', type=float, default=1e-5)
    parser.add_argument('--max_epoch', '-E', type=int, default=100)

    parser.add_argument('--hidden_width_scaler', type=int, default = 10)
    parser.add_argument('--hidden_depth', type=int, default = 3)
    parser.add_argument('--Cuda_number', type=int, default = 0)
    parser.add_argument('--num_of_batch', type=int, default=10)
    parser.add_argument('--log_interval', type=int, default=100)
    parser.add_argument('--VISUALIZATION', action='store_true', default=False)
    args = parser.parse_args()
    
    print("Start...%s"%args)
    #Get data:
    data = data_stuff.get_standarlized_data()
    batch_size = data.shape[0]
    Y = data['need_to_compensate']
    #Input variables for models:
    #When in plan we only have:
    _X_planned = np.array([data['axc_speed_3'], data['axc_torque_ffw_gravity_3'], data['Temp'], data['axc_pos_3']])   #TODO: this is axis 4
    #In use we MUST have real_time_feedback, compensate from axc_torque to feedback_torque:
    _X_feedbacked = np.array([data['servo_feedback_speed_3'], data['axc_torque_ffw_gravity_3'], data['Temp'], data['servo_feedback_pos_3']])  #TODO: this is for axis 4
    X = np.insert(_X_feedbacked, 0, 1, axis=0)
    normer = data_stuff.normalizer(X, Y)
    X, Y = normer.normalize(X, Y)
    #embed()

    #---------------------------------Do polyfit---------------------------:
    #Init unknown params:
    if args.mode == 'linear':
        names_note = ['c0', 'c1', 'c2', 'c3', 'c4']
        params = np.array([-1.83062977e-09, 1.50000000e+00, 6.82213855e-09, 3.00000000e+00, -4.12101372e-09])
        params = np.array([1,1,1,1,1])
        opt, cov = curve_fit(classical_model.linear_friction_model, X, Y, maxfev=500000)
    else:
        names_note = ['c0', 'v_brk', 'F_brk', 'F_C', 'c1', 'c2', 'c3', 'c4']
        params = np.array([-1.78197004e-09, 1.99621299e+00, 9.82765471e-08, -6.02984398e-09, 1.49999993e+00, 1.78627117e-09, 5.00000000e+00, 2.69899575e-09])
        params = np.array([1,1,1,1,1,1,1,1])
        opt, cov = curve_fit(classical_model.nonlinear_friction_model, X, Y, maxfev=500000)
    poly_loss, _J, poly_friction_predicted, _, _ = classical_model.compute_loss(Y, X, opt, args)
    error_ratio = evaluate_error_rate(poly_friction_predicted, data, normer, showup=True)
    plot_utils.visual(Y.values, poly_friction_predicted, "poly", args, title=error_ratio)
    print("Names note:", names_note, 'J:', _J)
    print("Poly:", opt)
    print("Error rate:", error_ratio)

    #embed()
    #sys.exit()

    #--------------------------Do Batch gradient descent---------------------:
    #J_history = []
    #if VISUALIZATION:
    #    fig=plt.figure()
    #BGD_lr = args.learning_rate
    #for iters in range(int(args.max_epoch)):
    #    BGD_loss, J, BGD_friction_predicted, params, partials = classical_model.compute_loss(Y, X, params, args)
    #    gradients = np.dot(partials, BGD_loss)/batch_size
    #    params = params - BGD_lr*gradients
    #    if iters>10:
    #        if np.abs(J-J_old)<args.criterion:
    #            BGD_lr *= 0.2
    #            args.criterion *= 0.2
    #            print("Epoch:", iters, "Reducing lr to %s, and setting criterion to %s"%(BGD_lr, args.criterion))
    #            if BGD_lr < 1e-9:
    #               break
    #            pass
    #    J_old = J
    #    print(iters, 'of', args.max_epoch, J)
    #    J_history.append(J)
    #    error_ratio = evaluate_error_rate(BGD_friction_predicted, data, normer)
    #    plot_utils.visual(Y.values, BGD_friction_predicted, 'BGD', args, title=error_ratio)
    #print("Names note:", names_note)
    #print("BGD:", params)
    #print("Error rate:", error_ratio)
    #embed()
    #sys.exit()

    #-------------------------------------Do NN--------------------------------:
    #Data:
    _X = _X_feedbacked.T
    Y = Y.values.reshape(-1, 1)
    nn_X = torch.autograd.Variable(torch.FloatTensor(_X))
    nn_Y = torch.autograd.Variable(torch.FloatTensor(Y))
    #mannual split dataset:
    all_index = list(range(len(nn_Y)))
    val_index = list(range(int(len(nn_Y)*(0.5-args.test_ratio/2)), int(len(nn_Y)*(0.5+args.test_ratio/2))))
    train_index = list(set(all_index) - set(val_index))
    nn_X_train = nn_X[train_index]
    nn_Y_train = nn_Y[train_index]
    nn_X_val = nn_X[val_index]
    nn_Y_val = nn_Y[val_index]
    train_dataset = Data.TensorDataset(nn_X_train, nn_Y_train)
    validate_dataset = Data.TensorDataset(nn_X_val, nn_Y_val)
    train_loader = Data.DataLoader( 
            dataset=train_dataset, 
            batch_size=int(len(train_dataset)/args.num_of_batch),
            shuffle=True,
            drop_last=True,
	        num_workers=4,
            pin_memory=True
            )
    validate_loader = Data.DataLoader( 
            dataset=validate_dataset, 
            batch_size=int(len(validate_dataset)/args.num_of_batch),
            shuffle=True,
            drop_last=True,
	        num_workers=4,
            pin_memory=True
            )
    #Model:
    device = torch.device("cuda", args.Cuda_number)
    #device = torch.device("cpu")
    input_size = nn_X.shape[1]                          
    hidden_size = nn_X.shape[1]*args.hidden_width_scaler
    hidden_depth = args.hidden_depth
    output_size = nn_Y.shape[1]
    model = NN_model.NeuralNet(input_size, hidden_size, hidden_depth, output_size, device)
    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)
    embed()
    #Train and Validate:
    train_loss_history = []
    validate_loss_history = []
    for epoch in range(int(args.max_epoch+1)):
        NN_friction_predicted = np.array(model.cpu()((nn_X)).detach()).reshape(-1)
        error_ratio_val = evaluate_error_rate(NN_friction_predicted[val_index], data.iloc[val_index], normer)
        error_ratio_train = evaluate_error_rate(NN_friction_predicted[train_index], data.iloc[train_index], normer)
        plot_utils.visual(Y, NN_friction_predicted, 'NN', args, extra=[train_index, val_index], title=error_ratio_val)

        train_loss, train_outputs, train_targets = train(args, model, device, train_loader, optimizer, epoch)
        validate_loss, validate_outputs, validate_targets = validate(args, model, device, validate_loader)
        train_loss_history.append(train_loss)
        validate_loss_history.append(validate_loss)
        train_loss_history[0] = validate_loss_history[0]
        print("Epoch: %s"%epoch)
        print("Train set  Average loss: {:.8f}".format(train_loss), "error ratio:", error_ratio_train)
        print('Validate set Average loss: {:.8f}'.format(validate_loss), "error ratio:", error_ratio_val)
        #embed()
    if args.VISUALIZATION:
        plt.plot(train_loss_history)
        plt.plot(validate_loss_history)

    names_note = "NN weights"
    print("Names note:", names_note)
    print("NN:", "NONE")
    print("Error rate:", error_ratio)

    embed()
    sys.exit()
   
import torch.nn as nn

class NeuralNet(nn.Module):
    def __init__(self, input_size, hidden_size, hidden_depth, output_size, device):
        super(NeuralNet, self).__init__()
        self.fc_first1 = nn.Linear(input_size, 10)
        self.fc_first2 = nn.Linear(10, hidden_size)
        self.hidden_depth = hidden_depth
        self.fc_last1 = nn.Linear(hidden_size, 10)
        self.fc_last2 = nn.Linear(10, output_size)
        self.relu = nn.ReLU()
        self.sp = nn.Softplus()
        self.th = nn.Tanh()
        self.ths = nn.Tanhshrink()
        self.sg = nn.Sigmoid()
        self.fcs = nn.ModuleList()   #collections.OrderedDict()
        self.bns = nn.ModuleList()   #collections.OrderedDict()
        for i in range(self.hidden_depth):
            self.bns.append(nn.BatchNorm1d(hidden_size, track_running_stats=True).to(device))
            self.fcs.append(nn.Linear(hidden_size, hidden_size).to(device))
    def forward(self, x):
        out = self.fc_first1(x)
        out = self.fc_first2(out)
        for i in range(self.hidden_depth):
            out = self.bns[i](out)
            out = self.fcs[i](out)
            out = self.relu(out)
        #out = self.th(out)
        out = self.fc_last1(out)
        out = self.fc_last2(out)
        out = self.ths(out)
        return out


'''#-*- coding:utf-8 -*-'''
import warnings
#warnings.filterwarnings("ignore")
import matplotlib.pyplot as plt
from IPython import embed
import numpy as np
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.cm as cm
import pandas as pd
from matplotlib import rcParams
title_size = 8
#----------------------------fuchen AQI project
mark_points = ['or', 'ob', 'og', 'ok', '^r', '+r', 'sr', 'dr', '<r', 'pr']
mark_centers = ['Dr', 'Db', 'Dg', 'Dk', '^b', '+b', 'sb', 'db', '<b', 'pb']
def rgb2gray(rgb):
    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]
    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b
    return gray

def svg_to_emf(who):
    os.system("inkscape -z %s.svg -M %s.emf"%who)
    return

def plot_scatter(data,labels,title,_plot=True):
    break_index = np.hstack((np.where(labels[:-1]-labels[1:]!=0)[0],data.shape[0]-1))
    if _plot:
        start = 0
        for i in break_index:
            plt.scatter(data[start:i,0],data[start:i,1])
            start = i+1
        plt.savefig("./pngs/%s.png"%title)
        plt.close()
    return

def plot_show(data,label_pred,centroids,inertia,title,_plot=True):
    if _plot:
        n_clusters = len(list(set(label_pred)))
        ax1 = plt.subplot(121)
        for i in range(n_clusters):
            ax1.plot(data[np.where(label_pred==i)][:,0],data[np.where(label_pred==i)][:,1],mark_points[i])
        if centroids is not None:
            for i in range(n_clusters):
                ax1.plot(centroids[i][0],centroids[i][1],mark_centers[i])
        ax2 = plt.subplot(122)
        ax2.imshow(label_pred.reshape(4*5,-1))
        plt.title(title)
        plt.savefig("./pngs/%s.png"%title)
        plt.close()
    return

def plot_radar(data,col_names,title,_plot):
    if _plot:
        import radar_chart   # radar plot is implemented in another .py
        N_features = data.shape[1]
        data = [list(col_names),data]
        radar_chart.start(data,N_features,title)
    else:
        pass
    return

def plot_mat(data,title,labels,shuffle_within_cluster=False,_plot=True):
    if _plot:
        index_ = np.array(list(range(labels.shape[0])))
        split_index = np.hstack((np.where(labels!=np.delete(np.insert(labels,0,999),-1))[0]-1,labels.shape[0]-1))
        if shuffle_within_cluster:
            for i in range(split_index.shape[0]-1):
                np.random.shuffle(index_[split_index[i]+1:split_index[i+1]+1])
            data = data[index_]
    
        plt.matshow(data, cmap=plt.cm.Blues)
        plt.title("%s"%title)
        plt.plot([data.shape[1],-1],[list(split_index+0.5),list(split_index+0.5)],color='red',alpha=0.65,lw=3)
        plt.xlim(0,data.shape[1])
        plt.ylim(0,data.shape[0])
        plt.savefig("./pngs/%s.png"%title)
        plt.close()
    else:
        pass
    return

def plot_pd_corr(finals, title, _plot=True):
    if _plot:
        plt.figure(figsize=(14, 12))
        plt.title("%s"%title, y=1.03, size=15)
        sns.heatmap(
            np.abs(finals.corr()),
            linewidths=0.1,
            vmax=1.0,
            vmin = 0,
            cmap='inferno',
            square=True,
            linecolor='white',
            annot=True)
        plt.xticks(rotation=90) 
        plt.yticks(rotation=360)
        plt.savefig("./pngs/%s.png"%title)
        plt.close()
    return

#--------------------wangke car_face project
def seat_prediction_vs_seat_target(outputs_left, labels_left, outputs_right, labels_right, outputs_merged, labels_merged):
#    outputs_left =  outputs_left.detach().cpu().numpy()
#    labels_left = labels_left.detach().cpu().numpy()
#    outputs_right =  outputs_right.detach().cpu().numpy()
#    labels_right = labels_right.detach().cpu().numpy()
    plt.clf()
    ax1 = plt.subplot(321)
    ax2 = plt.subplot(322)
    ax3 = plt.subplot(323)
    ax4 = plt.subplot(324)
    ax5 = plt.subplot(325)
    ax6 = plt.subplot(326)

    xticks = [str(i) for i in range(1,1+outputs_left.shape[1])]
    sns.heatmap(outputs_left, linewidths=0.1, vmin=0, vmax=1, cmap='Reds', square=True, linecolor='white', annot=True, ax=ax1, xticklabels=xticks)
    sns.heatmap(labels_left, linewidths=0.1, vmin=0, vmax=1, cmap='Reds', square=True, linecolor='white', annot=True, ax=ax2, xticklabels=xticks)
    sns.heatmap(outputs_right, linewidths=0.1, vmin=0, vmax=1, cmap='Greens', square=True, linecolor='white', annot=True, ax=ax3, xticklabels=xticks)
    sns.heatmap(labels_right, linewidths=0.1, vmin=0, vmax=1, cmap='Greens', square=True, linecolor='white', annot=True, ax=ax4, xticklabels=xticks)
    sns.heatmap(outputs_merged, linewidths=0.1, vmin=0, vmax=1, cmap='Blues', square=True, linecolor='white', annot=True, ax=ax5, xticklabels=xticks)
    sns.heatmap(labels_merged, linewidths=0.1, vmin=0, vmax=1, cmap='Blues', square=True, linecolor='white', annot=True, ax=ax6, xticklabels=xticks)

    ax1.set_title("Prediction", fontsize=title_size)
    ax2.set_title("Target", fontsize=title_size)
    ax3.set_title("Prediction", fontsize=title_size)
    ax4.set_title("Target", fontsize=title_size)
    ax5.set_title("Merged prediction", fontsize=title_size)
    ax6.set_title("Reality", fontsize=title_size)
    ax1.set_xlabel("SeatsNO");ax1.set_ylabel("LeftCamView")
    ax2.set_xlabel("SeatsNO");
    ax3.set_xlabel("SeatsNO");ax3.set_ylabel("RightCamView")
    ax4.set_xlabel("SeatsNO");
    ax5.set_xlabel("SeatsNO");ax5.set_ylabel("BothViewMerged")
    ax6.set_xlabel("SeatsNO");
    plt.draw()
    plt.pause(0.001)
    return    

def animation_of_train_and_validation_loss_and_seat_prediction_vs_seat_target(history_loss, v_history_loss, outputs, labels):
    outputs =  outputs.detach().cpu().numpy()
    labels = labels.detach().cpu().numpy()
    reshaper = np.int(np.floor((np.sqrt(outputs.shape[0]))))
    outputs = outputs[:reshaper**2].reshape(reshaper, reshaper)
    labels = labels[:reshaper**2].reshape(reshaper, reshaper)
    plt.clf()
    #AX1
    ax1 = plt.subplot(211)
    xaxis_train = range(len(history_loss))
    xaxis_validate = range(len(v_history_loss))
    scope = 50
    ax1.scatter(xaxis_train[-scope:], 100*np.array(history_loss[-scope:]), color='k', s=1.5)
    ax1.scatter(xaxis_validate[-scope:], 100*np.array(v_history_loss[-scope:]), color='blue', s=1.5)
    ax1.plot(xaxis_train[-scope:], 100*np.array(history_loss[-scope:]), color='k', label='trainloss')
    ax1.plot(xaxis_validate[-scope:], 100*np.array(v_history_loss[-scope:]), color='blue', label="validateloss")
    ax1.set_xlabel("Epoches")
    ax1.set_ylabel("Loss")
    ax1.legend()
    ax1.set_title("Train and Validation Loss", fontsize=title_size)
    #AX2
    ax2 = plt.subplot(245)
    xticks = [str(i) for i in range(1,1+outputs.shape[1])]

    sns.heatmap(outputs, linewidths=0.1, vmin=0, vmax=int(labels.max()), cmap='Blues', square=True, linecolor='white', annot=True, ax=ax2, xticklabels=xticks)
    ax2.set_title("Prediction on last validation batch", fontsize=title_size)
    ax2.set_xlabel("Sample");ax2.set_ylabel("Sample")
    #AX3
    ax3 = plt.subplot(246)
    sns.heatmap(labels, linewidths=0.1, vmin=0, vmax=int(labels.max()), cmap='Blues', square=True, linecolor='white', annot=True, ax=ax3, xticklabels=xticks)
    ax3.set_title("Ground truth\n(different number refers to different position)", fontsize=title_size)
    ax3.set_xlabel("Sample");ax3.set_ylabel("Sample")
    #AX4
    ax3 = plt.subplot(247)
    sns.heatmap(np.abs(labels-outputs), linewidths=0.1, vmin=0, vmax=int(labels.max()), cmap='Blues', square=True, linecolor='white', annot=True, ax=ax3, xticklabels=xticks)
    ax3.set_title("Errors values abs(labels-outputs)", fontsize=title_size)
    ax3.set_xlabel("Sample");ax3.set_ylabel("Sample")
    #AX5
    ax3 = plt.subplot(248)
    wrongs = abs(np.round(outputs)-labels)
    sns.heatmap(wrongs, linewidths=0.1, vmin=0, vmax=int(labels.max()), cmap='Reds', square=True, linecolor='white', annot=True, ax=ax3, xticklabels=xticks)
    ax3.set_title("Wrongs: %s"%wrongs.sum(), fontsize=title_size)
    ax3.set_xlabel("Sample");ax3.set_ylabel("Sample")

    plt.draw()
    plt.pause(0.001)
    return   

def animation_of_train_and_test_loss_and_curve_prediction_vs_curve_target( \
        history_loss, v_history_loss, \
        v_outputs, v_labels, v_t_axis, \
        t_outputs, t_labels, t_t_axis, \
        c_outputs, c_labels, c_t_axis, \
        infos, title):
    (epoch, RR, new_psi) = infos
    window_length = 200
    plt.clf()
    ax1 = plt.subplot(131)
    ax2 = plt.subplot(132)
    ax3 = plt.subplot(133)

    #subplot 1:
    ax1.plot(np.log10(history_loss[-window_length:]), label="Trainning Loss")
    ax1.plot(np.log10(v_history_loss[-window_length:]), label="Validation Loss")
    ax1.legend()
    ax1.set_title("Logged Train and Validation Loss of %s PSI"%title, fontsize=title_size)
    ax1.set_xlabel("Number of epoch")
    ax1.set_ylabel("Loss over train and testset")
    if history_loss.shape[0]>window_length:
        ax1.set_xticklabels(np.linspace(epoch-window_length+1,\
			epoch+1,10).astype(int))

    #subplot 2:
    ax2.scatter(t_t_axis, t_outputs, \
            label = "Traindata Prediction", \
			s = 1, color = 'orange')
    ax2.scatter(t_t_axis, t_labels, \
			label="Traindata Labels",\
			s = 1, color = 'grey')
    ax2.scatter(v_t_axis, v_outputs, \
            label = "Testdata Prediction",\
			s = 1, color = 'blue',alpha=0.5)
    ax2.scatter(v_t_axis ,v_labels,\
            label="Testdata Labels",\
			s = 1, color = 'k', alpha=0.5)
    ax2.legend()
    ax2.set_title("Prediction of probability increments",\
			fontsize=title_size)
    ax2.set_xlabel("LogT time")
    ax2.set_ylabel("Probability of Damage")

    #subplot3, culmulative sum of alpha
    ax3.scatter(c_t_axis ,c_labels,\
            label="Labels for baseline data",\
			s = 1, color = 'grey', alpha=0.5)
    ax3.scatter(c_t_axis, c_outputs, \
            label = "Culmulative probability",\
			s = 0.8, color = 'red')
    ax3.legend()
    ax3.set_title("Performance on culmulative probability RR:{0:.4f}%".\
			format(RR*100), fontsize=title_size)
    ax3.set_xlabel("LogT time")
    ax3.set_ylabel("Probability of Damage(%)")
    
#    embed()
#    import os
#    name=str(int(os.popen("ls *.png|wc -l").read()))
#    plt.savefig(name.zfill(3)+".png")
    plt.draw()
    plt.pause(0.001)
    return    

def damage_prediction_test_time( \
        damage_curve,\
        c_outputs, c_labels, c_t_axis, \
        n_outputs, n_labels, n_t_axis, \
        infos, status, outputname='out', filetype='pdf',\
        ):
    (markers, RR, psi, mean, cov,alters) = infos
    plt.rcParams["font.family"] = "Times New Roman"
#    plt.rcParams["font.family"] = "Fantasy"
#    rcParams.update({
#		    'font.family':'Times New Roman',
#			'font.sans-serif':['Liberation Sans'],
#				})
    plt.figure(figsize=(24,12))
    ax1 = plt.subplot(121)
    ax2 = plt.subplot(122)
    size = 21
    textsize = size
    headnamesize = size+10
    x_pos = 7.0
    n_outputs *= 1.045
    #LEFT FIG-----------------------------------------------------------------
    df = pd.DataFrame([])
    df_names = []
    for index,_ in enumerate(alters[:-1]):
        ax1.scatter(n_t_axis[alters[index]:alters[index+1]], \
				damage_curve[alters[index]:alters[index+1]],\
				label="Damage rate of:"+str(int(markers[index][-1]))+" psi, wood:("+str(markers[index][2])+","+str(markers[index][1])+")",\
		    s=4 , cmap='Blues')
        #save_data left_for_tao:
        case_string = str(int(markers[index][-1]))+" psi, wood:("+str(markers[index][2])+","+str(markers[index][1])+")"
        #df = pd.DataFrame({\
        #           case_string+'x':n_t_axis[alters[index]:alters[index+1]].reshape(-1),\
        #           case_string+'y':damage_curve[alters[index]:alters[index+1]].reshape(-1)\
        df = pd.concat([df, pd.DataFrame(n_t_axis[alters[index]:alters[index+1]].reshape(-1))],ignore_index=True,axis=1)
        df = pd.concat([df, pd.DataFrame(damage_curve[alters[index]:alters[index+1]].reshape(-1))],ignore_index=True,axis=1)
        df_names.append(case_string+'x')
        df_names.append(case_string+'y')
    df.columns = df_names
    df.to_csv(outputname+"_left.csv", line_terminator="\r\n")

    ax1.legend(prop={'size':size})
#    ax1.set_title("Prediction of probability increments",size=size+5)
    ax1.set_xlabel("Log-time (log hours)",size=size+5,labelpad=10)
    ax1.set_ylabel("Increments of Damage",size=size+5,labelpad=10)
    ax1.tick_params(labelsize=size)
    ax1.text(ax1.get_xbound()[0]+(ax1.get_xbound()[1]-ax1.get_xbound()[0])/25,\
			ax1.get_ybound()[1]-(ax1.get_ybound()[1]-ax1.get_ybound()[0])/15,\
			"a)",fontsize=headnamesize)

    #RIGHT FIG----------------------------------------------------------------
    ax2.scatter(c_t_axis ,c_labels,\
            label = "Basic",\
			s = 3, color = 'grey')
    ax2.scatter(n_t_axis, n_outputs, \
			label = status,\
			s = 3, color = 'red')
    #save_data right_for_tao:
    names = []
    right_datas = pd.DataFrame([])
    seqs = np.hstack((np.insert(np.where(np.diff(c_t_axis)<0),0,-1),np.array(-1)))
    for index,seq in enumerate(seqs): 
        part1 = pd.DataFrame([])
        part2 = pd.DataFrame([])
        part1_red = pd.DataFrame([])
        part2_red = pd.DataFrame([])
        if index==len(seqs)-1:
           break
        part1["x_gray_of_line_%s"%index] = c_t_axis[seqs[index]+1:seqs[index+1]]
        right_datas = pd.concat([right_datas, part1], ignore_index=True, axis=1)
        part2["y_gray_of_line_%s"%index] = c_labels[seqs[index]+1:seqs[index+1]]
        right_datas = pd.concat([right_datas, part2], ignore_index=True, axis=1)
        names.append("x_gray_of_line_%s"%index)
        names.append("y_gray_of_line_%s"%index)
        if status != "Generalized":
            part1_red['x_red_of_line_%s'%index] = n_t_axis[seqs[index]+1:seqs[index+1]]
            right_datas = pd.concat([right_datas, part1_red], ignore_index=True, axis=1)
            part2_red['y_red_of_line_%s'%index] = n_outputs[seqs[index]+1:seqs[index+1]]
            right_datas = pd.concat([right_datas, part2_red], ignore_index=True, axis=1)
            names.append("x_red_of_line_%s"%index)
            names.append("y_red_of_line_%s"%index)
    if status == "Generalized":
        part1_red = pd.DataFrame([])
        part2_red = pd.DataFrame([])
        part1_red['x_red_of_generalized'] = n_t_axis
        right_datas = pd.concat([right_datas, part1_red], ignore_index=True, axis=1)
        part2_red['y_red_of_generalized'] = n_outputs
        right_datas = pd.concat([right_datas, part2_red], ignore_index=True, axis=1)
        names.append("x_red_of_generalized")
        names.append("y_red_of_generalized")
    right_datas.columns = names
    right_datas.to_csv(outputname+"_right.csv", line_terminator="\r\n")
    for marker in markers:
#        ax1.scatter(7.5, marker[0], s=15, color='green')
        if int(marker[-1])==3000:
            ax2.text(x_pos, marker[0], \
				str(marker[-1])+", ("+str(marker[2])+","+str(marker[1])+")",\
				fontsize=size*2/3)
        else:
            ax2.text(x_pos, marker[0], \
				str(marker[-1])+", ("+str(marker[2])+","+str(marker[1])+")",\
				fontsize=textsize)
    if status == "Generalized":
        ax2.text(x_pos, n_outputs[-1]-0.02, "%s, (%s,%s)"%(psi, mean, cov),\
				fontsize=textsize, color='red')
    ax2.legend(loc=(0.03,0.8),prop={'size':size})
#    ax2.set_title("Performance of M-D model (test RR:{0:.1f}%)".format(RR*100),\
#			size=size+5)
    ax2.set_xlabel("Log-time (log hours)",size=size+5,labelpad=10)
    ax2.set_ylabel("Probability of Damage",size=size+5,labelpad=10)
    ax2.tick_params(labelsize=size)
    ax2.text(ax2.get_xbound()[0]+(ax2.get_xbound()[1]-ax2.get_xbound()[0])/25,\
			ax2.get_ybound()[1]-(ax2.get_ybound()[1]-ax2.get_ybound()[0])/15,\
			"b)",fontsize=headnamesize)
    plt.savefig("%s.%s"%(outputname,filetype),format='%s'%filetype)
    plt.show()
    return    


def prediction_vs_real( \
                t_outputs_close_loop, t_outputs_open_loop,\
                t_labels, t_t_axis, \
                infos, title):
    RR = infos
    window_length = 40
    plt.clf()
    ax1 = plt.subplot(111)
    ax1.scatter(t_t_axis, t_outputs_close_loop, \
            label = "PredictionCloseLoop", \
			s = 1, color = 'red')
    ax1.scatter(t_t_axis, t_outputs_open_loop, \
            label = "PredictionOpenLoop", \
			s = 1, color = 'green',alpha=0.5)
    ax1.scatter(t_t_axis, t_labels, \
			label="Labels",\
			s = 1, color = 'k')
    ax1.legend()
    ax1.set_title("Probability predictionof {} PSI, RR:{}%".\
			format(title, np.round(RR*100,4)),\
			fontsize=title_size)
    ax1.set_xlabel("Number of sampled data")
    ax1.set_ylabel("Probability")
    
    plt.show()
    return    

class dict_upgrader:     #Upgrade dict to class
    def __init__(self, **entries):
        self.__dict__.update(entries)

def sort_labels_and_handles_by_labels(ax):                                  
    handles, labels = ax.get_legend_handles_labels()
    indexer = np.array(np.argsort([float(label.split(':')[-1]) for label in labels]))
    labels, handles = list(np.array(labels)[indexer]),list(np.array(handles)[indexer])
    return labels, handles

# evaluation of cross validation plot:
def historical_best(seq):
    seq = list(seq)
    for index,value in enumerate(seq[:-1]):
        if seq[index+1]>seq[index]:
            seq[index+1] = min(seq[:index+1])
    return np.array(seq)
def sort_on_name():
    sorted_files =\
        list(np.array(model_files)[np.argsort([float(re.findall("%s_(.*?)_"%args.who.capitalize(),\
        tmp)[0]) for tmp in model_files])])
    return sorted_files
def make_plot():
    colors = plt.cm.jet(np.linspace(1,0,num_of_experiments))
    ax1 = plt.subplot(121)
    ax2 = plt.subplot(122)
    color_counter = 1
    for model_file in sorted_files:
        print("On:", model_file)
        varies_value = float(re.findall("%s_(.*?)_"%args.who.capitalize(), model_file)[0])
        if args.who == "hidden":
            net = model.LogTProFC(nn_input_dims, hidden_scaler=int(varies_value))
        else:
            net = model.LogTProFC(nn_input_dims,\
                    hidden_scaler=int(fixed_hidden_size))
        m = torch.load(model_dir+model_file)
        try:
            net.load_state_dict(m['net'])
        except:
            print("Error loading model file...")
            embed()
        training_history_loss = m['training_history_loss']
        test_history_loss = m['test_history_loss']
        loss = test_history_loss
        ax1.plot(np.log10(range(1,len(training_history_loss)+1)), \
                loss, label="%s value:%s"%(args.who, varies_value), color =\
                colors[color_counter])
        ax1.set_title("Trace of network performance over %s"%args.who)
        ax1.set_xlabel("Log Epoch Number")
        ax2.plot(np.log10(range(1,len(training_history_loss)+1)), \
                historical_best(loss), label="%s value:%s"%(args.who,\
                    varies_value),\
                color = colors[color_counter])
        ax2.set_title("Historical best performance over %s"%args.who)
        ax2.set_xlabel("Log Epoch Number")
        color_counter += 1

        labels, handles = plot_utils.sort_labels_and_handles_by_labels(ax1)
        ax1.legend(handles, labels)
        labels, handles = plot_utils.sort_labels_and_handles_by_labels(ax2)
        ax2.legend(handles, labels)
        del(net)
        del(m)
    plt.show()


#===
#    cbar = plt.colorbar(tmp)
#    cbar.set_ticks(classid.values())
#    cbar.set_ticklabels(classid.keys())

def scatter3d(single_class, class_i,ax,lb,ub):
    ###plot points:
    size = 1 if class_i == "DontCare" else 3
    if size == 1:
        ###continue
        pass
    tmp=ax.scatter3D(single_class[:,0], single_class[:,1], single_class[:,2],\
			s=single_class[:,3]*4, label=class_i)
    ax.set_xlim(lb,ub)
    ax.set_ylim(lb,ub)
    ax.set_zlim(lb,ub)
   #ax.set_zbound(min(ax.xy_dataLim.xmin,ax.xy_dataLim.ymin),max(ax.xy_dataLim.xmax,ax.xy_dataLim.ymax))

    ax.set_xlabel("x");ax.set_ylabel("y")
    ax.legend()
    return

def scatter_flat(points):
#    ax0 = plt.subplot(411)
    ax1 = plt.subplot(311)
    ax2 = plt.subplot(312)
    ax3 = plt.subplot(313)

#    ax0.tricontour(points[:,0],points[:,1],points[:,2],100,cmap=plt.cm.jet)
    ax1.scatter(points[:,0], points[:,1], s=1, c=points[:,2])
    ax1.set_title("Color of Height(m)")
    ax2.scatter(points[:,0], points[:,1], s=1, c=points[:,3])
    ax2.set_title("Color of Intensity")
    ax3.scatter(points[:,0], points[:,1], s=1, c=points[:,4])
    ax3.set_title("Color of class")
    return

def hist_distribution(data):
    ###plot bar distribution:
    data = pd.DataFrame(data)
    data.plot.hist(bins=250)
    return


#----------Friction compensate:
def visual(Y, predicted, method, args, extra=None, title=None):
    if args.VISUALIZATION:
        plt.clf()
        predicted = np.array(predicted.reshape(-1))
        #plt.plot(Y.reshape(-1)[::100], label='real')
        #plt.plot(predicted[::100], label='%s predicted'%method)
        if extra==None:
            plt.scatter(range(len(Y.reshape(-1)[::10])), Y.reshape(-1)[::10], label='real', s=2)
            plt.scatter(range(len(predicted[::10])), predicted[::10], label='%s predicted'%method, s=2)
        else:
            train_index = extra[0]
            val_index = extra[1]
            plt.scatter(range(len(Y.reshape(-1)))[::10], Y.reshape(-1)[::10], label='real', s=2)
            plt.scatter(np.array(range(len(predicted)))[train_index][::10], predicted[train_index][::10], label='trainset %s predicted'%method, s=2)
            plt.scatter(np.array(range(len(predicted)))[val_index][::10], predicted[val_index][::10], label='valset %s predicted'%method, s=2)
        plt.title("Relative error rate: low:{:.2f}% high:{:.2f}%".format(np.array(title[0]), np.array(title[1])))
        plt.ylim(-3, 3)
        plt.legend()
        if method!="poly":
            plt.draw()
            plt.pause(1e-10)
        else:
            plt.show()

def report_it(func):
    def run(*argv):
        if DEBUG:
            print (func.__name__, "called")
        if argv:
            ret = func(*argv)
        else:
            ret = func()
        return ret
    return run

def savitzky_golay(y, window_size, order, deriv=0, rate=1):

    import numpy as np
    from math import factorial

    try:
        window_size = np.abs(np.int(window_size))
        order = np.abs(np.int(order))
    except ValueError:
        raise ValueError("window_size and order have to be of type int")
    if window_size % 2 != 1 or window_size < 1:
        raise TypeError("window_size size must be a positive odd number")
    if window_size < order + 2:
        raise TypeError("window_size is too small for the polynomials order")
    order_range = range(order+1)
    half_window = (window_size -1) // 2
    # precompute coefficients
    b = np.mat([[k**i for i in order_range] for k in range(-half_window, half_window+1)])
    m = np.linalg.pinv(b).A[deriv] * rate**deriv * factorial(deriv)
    # pad the signal at the extremes with
    # values taken from the signal itself
    firstvals = y[0] - np.abs( y[1:half_window+1][::-1] - y[0] )
    lastvals = y[-1] + np.abs(y[-half_window-1:-1][::-1] - y[-1])
    y = np.concatenate((firstvals, y, lastvals))
    return np.convolve( m[::-1], y, mode='valid')
#coding:utf-8
#simple model
import time
from IPython import embed
import numpy as np
import pandas as pd
import os,sys,time
import matplotlib.pyplot as plt
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D
from scipy.optimize import curve_fit
import scipy
import argparse
import pandas as pd
from tqdm import tqdm

import pickle as pkl
import PIL

import torch.nn as nn
import torch
import torch.optim as optim
import torch.utils.data as Data
import torch.nn.functional as F
torch.manual_seed(44)

#plt.ion()
DEBUG = False
DEBUG = True
VISUALIZATION = False
VISUALIZATION = True

def report_it(func):
    def run(*argv):
        if DEBUG:
            print (func.__name__, "called")
        if argv:
            ret = func(*argv)
        else:
            ret = func()
        return ret
    return run

def train(args, model, device, train_loader, optimizer, epoch):
    model.train()
    LOSS = 0
    pbar = tqdm(train_loader)
    for batch_idx, (data, extra, target) in enumerate(pbar):
        data, extra, target = data.to(device), extra.to(device), target.to(device)
        optimizer.zero_grad()
        output = model((data, extra))
        loss = F.mse_loss(output, target)
        loss.backward()
        LOSS += F.mse_loss(output, target, reduction='sum').item() # pile up all loss
        optimizer.step()
        if (batch_idx % args.log_interval == 0 or batch_idx == len(train_loader)-1)and(batch_idx!=0):
            #pbar.set_description('Train Epoch: {} [{}/{} ({:.0f}%)]. Loss: {:.8f}'.format(epoch, batch_idx*len(data), len(train_loader.dataset), 100.*batch_idx/len(train_loader), loss.item()))
            pass
    train_loss_mean = LOSS/len(train_loader.dataset)
    print("Train Epoch: {} LOSS:{:.1f}, Average loss: {:.8f}".format(epoch, LOSS, train_loss_mean))
    print(output[0], target[0])
    return train_loss_mean

def validate(args, model, device, validate_loader):
    model.eval()
    LOSS = 0
    outputs_record = np.array([])
    targets_record = np.array([])
    with torch.no_grad():
        pbar = tqdm(validate_loader)
        for idx, (data, extra, target) in enumerate(pbar):
            data, extra, target = data.to(device), extra.to(device), target.to(device)
            output = model((data, extra))
            LOSS += F.mse_loss(output, target, reduction='sum').item() # pile up all loss
            outputs_record = np.append(outputs_record, output.cpu())
            targets_record = np.append(targets_record, target.cpu())
            #pbar.set_description('Validate: [{}/{} ({:.0f}%)]'.format(idx*len(data), len(validate_loader.dataset), 100.*idx/len(validate_loader)))
    validate_loss_mean = LOSS/len(validate_loader.dataset)
    print('Validate set LOSS: {:.1f}, Average loss: {:.8f}'.format(LOSS, validate_loss_mean))
    print(output[0], target[0])
    return validate_loss_mean, outputs_record, targets_record

'''
class MagNet(nn.Module):
    def __init__(self, input_size, output_size, device):
        super(MagNet, self).__init__()
        self.avg = nn.AvgPool2d(2)
        self.fc0 = nn.Linear(2500, 2500)
        self.bn0 = nn.BatchNorm1d(2500, track_running_stats=True)
        self.fc1 = nn.Linear(2500, 2500)
        self.bn1 = nn.BatchNorm1d(2500, track_running_stats=True)
        self.fc2 = nn.Linear(2500, 2500)
        self.bn2 = nn.BatchNorm1d(2500, track_running_stats=True)
        self.fc3 = nn.Linear(2500, 100)
        self.bn3 = nn.BatchNorm1d(100, track_running_stats=True)

        self.fc4 = nn.Linear(103, 103)
        self.bn4 = nn.BatchNorm1d(103, track_running_stats=True)
        self.fc5 = nn.Linear(103, 103)
        self.bn5 = nn.BatchNorm1d(103, track_running_stats=True)
        self.fc6 = nn.Linear(103, 20)
        self.bn6 = nn.BatchNorm1d(20, track_running_stats=True)
        self.fc7 = nn.Linear(20, 3)

        self.relu = nn.ReLU()
        self.sp = nn.Softplus()
        self.th = nn.Tanh()
        self.sg = nn.Sigmoid()
    def forward(self, X):
        x = X[0]
        x2 = X[1]
        out = self.avg(x)

        out = self.fc0(out.view(out.shape[0], -1))
        out = self.bn0(out)
        out = self.th(out)

        out = self.fc1(out)
        out = self.bn1(out)
        out = self.th(out)

        out = self.fc2(out)
        out = self.bn2(out)
        out = self.th(out)

        out = self.fc3(out)
        out = self.bn3(out)
        out = self.th(out)

        out = torch.cat((out,x2), 1)

        out = self.fc4(out)
        out = self.bn4(out)
        out = self.th(out)

        out = self.fc5(out)
        out = self.bn5(out)
        out = self.th(out)

        out = self.fc6(out)
        out = self.bn6(out)
        out = self.th(out)

        out = self.fc7(out)
        out = self.sp(out)
        return out
'''

class NeuralNet(nn.Module):
    def __init__(self, input_size, hidden_size, hidden_depth, output_size, device):
        super(NeuralNet, self).__init__()
        self.fc_first1 = nn.Linear(input_size, 10)
        self.fc_first2 = nn.Linear(10, hidden_size)
        self.hidden_depth = hidden_depth
        self.fc_last1 = nn.Linear(hidden_size, 10)
        self.fc_last2 = nn.Linear(10, output_size)
        self.relu = nn.ReLU()
        self.sp = nn.Softplus()
        self.th = nn.Tanh()
        self.sg = nn.Sigmoid()
        self.fcs = nn.ModuleList()   #collections.OrderedDict()
        self.bns = nn.ModuleList()   #collections.OrderedDict()
        for i in range(self.hidden_depth):
            self.bns.append(nn.BatchNorm1d(hidden_size, track_running_stats=True).to(device))
            self.fcs.append(nn.Linear(hidden_size, hidden_size).to(device))
    def forward(self, x):
        out = self.fc_first1(x)
        out = self.fc_first2(out)
        for i in range(self.hidden_depth):
            out = self.bns[i](out)
            out = self.fcs[i](out)
        out = self.th(out)
        out = self.fc_last1(out)
        out = self.fc_last2(out)
        out = self.sp(out)
        return out

def get_data():
    data = pd.read_csv("./realtime-20190827-165608.rec-data.prb-log", sep=' ', index_col='line')
    data = data.drop("start_tick").astype(float)
    size = int(1e3)
    column_names = ['v', 'q', 'Temp', 'fai_a', 'fai_b', 'fai_c', 'fai_d', 'fai_e', 'fai_f', 'tao_f', 'tao_t_g', 'tao_t_ng']
    data = pd.DataFrame(np.random.random((size, len(column_names))), columns=column_names)
    friction_value = data['v']*1.5 + data['Temp']*5 + 0*np.random.rand(size)
    data['friction_values'] = friction_value
    return data

def linear_friction_model(X, c0, c1, c2, c3, c4):
    f = X[0]*c0 + X[1]*c1 + X[2]*c2 + X[3]*c3 + X[4]*c4
    return f

def linear_friction_model_derivatives(X):
    partial_f_c0 = X[0]
    partial_f_c1 = X[1]
    partial_f_c2 = X[2]
    partial_f_c3 = X[3]
    partial_f_c4 = X[4]
    partials = np.vstack((partial_f_c0, partial_f_c1, partial_f_c2, partial_f_c3, partial_f_c4))
    return partials 

def nonlinear_friction_model(X, c0, v_brk, F_brk, F_C, c1, c2, c3, c4):
    f = X[0]*c0 + (np.sqrt(2*np.e)*(F_brk-F_C)*np.exp(-(X[1]/(v_brk*np.sqrt(2)))**2)*(X[1]/(v_brk*np.sqrt(2))) + F_C*np.tanh(X[1]/(v_brk/10)) + X[1]*c1) + X[2]*c2 + X[3]*c3 + X[4]*c4
    return f

def nonlinear_friction_model_derivatives(X, v_brk, F_C, F_brk):
    partial_f_c0 = X[0]
    partial_f_v_brk = -10*F_C*X[1]*(1 - np.tanh(10*X[1]/v_brk)**2)/v_brk**2 + (1/np.sqrt(2))*X[1]**3*((-np.sqrt(2*np.e))*F_C + (np.sqrt(2*np.e))*F_brk)*np.exp(-0.5*X[1]**2/v_brk**2)/v_brk**4 - (1/np.sqrt(2))*X[1]*((-np.sqrt(2*np.e))*F_C + (np.sqrt(2*np.e))*F_brk)*np.exp(-0.5*X[1]**2/v_brk**2)/v_brk**2
    partial_f_F_brk = (np.sqrt(2*np.e)/np.sqrt(2))*X[1]*np.exp(-0.5*X[1]**2/v_brk**2)/v_brk
    partial_f_F_C = -(np.sqrt(2*np.e)/np.sqrt(2))*X[1]*np.exp(-0.5*X[1]**2/v_brk**2)/v_brk + np.tanh(10*X[1]/v_brk)
    partial_f_c1 = X[1]
    partial_f_c2 = X[2]
    partial_f_c3 = X[3]
    partial_f_c4 = X[4]
    partials = np.vstack((partial_f_c0, partial_f_v_brk, partial_f_F_brk, partial_f_F_C, partial_f_c1, partial_f_c2, partial_f_c3, partial_f_c4))
    return partials 

def compute_loss(Y, X, params):
    c0, c1, c2, c3, c4 = params[0], params[1], params[2], params[3], params[4]
    if args.mode == 'linear':
        friction_predicted = linear_friction_model(X, c0, c1, c2, c3, c4)
        partials = linear_friction_model_derivatives(X)
    else:
        v_brk = params[5]
        F_brk = params[6]
        F_C = params[7]
        friction_predicted = nonlinear_friction_model(X, c0, v_brk, F_brk, F_C, c1, c2, c3, c4)
        partials = nonlinear_friction_model_derivatives(X, v_brk, F_C, F_brk)
    loss = friction_predicted - Y
    J = np.sum(loss**2)/(2*X.shape[1]) 
    return loss, J, friction_predicted, params, partials

def visual(_predicted):
    #Y = np.array(Y.reshape(-1))
    _predicted = np.array(_predicted.reshape(-1))
    if VISUALIZATION:
        plt.clf()
        ax = fig.add_subplot(121, projection='3d')
        ax.plot_trisurf(data['v'], data['Temp'], Y, cmap='summer', alpha=0.9, label='label')
        ax.plot_trisurf(data['v'], data['Temp'], _predicted, cmap='winter', alpha=0.6, label='predicted')
        ax.plot_trisurf(data['v'], data['Temp'], Y-_predicted, cmap='hot', alpha=0.3, label='error')
        ax.scatter3D(data['v'], data['Temp'], Y, color='k')
        ax.set_xlabel('v(rad/s)',fontsize =20)
        ax.set_ylabel('Temp(degree)',fontsize =20)
        ax.set_zlabel('friction(N.m)',fontsize =20)
        ax2 = fig.add_subplot(122)
        ax2.plot(np.array(range(len(J_history))), np.log(J_history))

        #plt.draw()
        #plt.pause(0.02)
        plt.show()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Friction.')
    parser.add_argument('--mode', '-M', default='linear')
    parser.add_argument('--learning_rate', '-LR', type=float, default=0.01)
    parser.add_argument('--criterion', '-C', type=float, default=1e-199)
    parser.add_argument('--max_epoch', '-E', type=int, default=300)

    parser.add_argument('--hidden_width_scaler', type=int, default = 10)
    parser.add_argument('--hidden_depth', type=int, default = 3)
    parser.add_argument('--Cuda_number', type=int, default = 0)
    parser.add_argument('--num_of_batch', type=int, default=10)
    parser.add_argument('--log_interval', type=int, default=100)
    args = parser.parse_args()
    
    print("Start...%s"%args)
    #Get data:
    data = get_data()
    batch_size = data.shape[0]
    Y = data['friction_values']
    #Input variables for models:
    _X = np.array([data['v'], data['tao_f'], data['Temp'], data['q']])
    X = np.insert(_X, 0, 1, axis=0)

    #---------------------------------Do polyfit---------------------------:
    if args.mode == 'linear':
        opt, cov = curve_fit(linear_friction_model, X, Y, maxfev=500000)
    else:
        opt, cov = curve_fit(nonlinear_friction_model, X, Y, maxfev=500000)
    poly_loss, _, poly_friction_predicted, _, _ = compute_loss(Y, X, opt)

    #--------------------------Do Batch gradient descent---------------------:
    #Init unknown params:
    if args.mode == 'linear':
        names_note = ['c0', 'c1', 'c2', 'c3', 'c4']
        params = np.array([1, 2, 3, 4, 5])
        params = np.array([-1.83062977e-09, 1.50000000e+00, 6.82213855e-09, 3.00000000e+00, -4.12101372e-09])
    else:
        names_note = ['c0', 'v_brk', 'F_brk', 'F_C', 'c1', 'c2', 'c3', 'c4']
        params = np.array([1, 2, 3, 4, 5, 1, 1, 1]) 
        params = np.array([-1.78197004e-09, 1.99621299e+00, 9.82765471e-08, -6.02984398e-09, 1.49999993e+00, 1.78627117e-09, 5.00000000e+00, 2.69899575e-09])
    J_history = []
    fig=plt.figure()
    BGD_lr = args.learning_rate
    for iters in range(int(args.max_epoch/100)):
        BGD_loss, J, BGD_friction_predicted, params, partials = compute_loss(Y, X, params)
        gradients = np.dot(partials, BGD_loss)/batch_size
        params = params - BGD_lr*gradients
        if iters>10:
            if np.abs(J-J_old)<args.criterion:
                BGD_lr = BGD_lr*0.2
                print("Epoch:", iters, "Reducing lr to %s, and setting criterion to %s"%(BGD_lr, args.criterion))
                if BGD_lr < 1e-6:
                   break
                pass
        J_old = J
        print(J)
        J_history.append(J)

    #-------------------------------------Do NN--------------------------------:
    #Data:
    #Magnet:
    '''
    min_speed = 0.3
    enough_shape = (123, 250)
    norm_size = (100, 100)
    tmp_data = pkl.load(open("/mfs/home/wangke/magnetic-detect/data/data0.pkl", 'rb'))
    imgs_data = []
    speeds = []
    shapes = []
    depths = []
    scalers = []
    for idx in range(len(tmp_data)):
        imgs_data.append(tmp_data[idx]['data'])
        speeds.append(tmp_data[idx]['speed']/min_speed)
        shapes.append(tmp_data[idx]['data'].shape)
        scalers.append(list(np.array(norm_size)/np.array(tmp_data[idx]['data'].shape)))
        depths.append(tmp_data[idx]['depth'])
    #substruct mean
    widths = []
    lengths = []
    for idx in range(len(tmp_data)):
        imgs_data[idx] = imgs_data[idx]-imgs_data[idx].mean()
        widths.append(tmp_data[idx]['width'])
        lengths.append(tmp_data[idx]['length'])
    #prepare paddings
    streched_data = []
    new_shapes = []
    pads_needed = []
    for i in range(len(tmp_data)):
        new_shape = np.array([int(shapes[i][0]), int(shapes[i][1]*speeds[i])])
        need_to_pad = (int((enough_shape[0]-new_shape[0])/2), int((enough_shape[1]-new_shape[1])/2))
        new_shapes.append(new_shape)
        pads_needed.append(need_to_pad)
        streched_data.append(np.array(PIL.Image.fromarray(imgs_data[i]).resize(norm_size, PIL.Image.BICUBIC)))
    #make pad
    padded_data = []
    varations = []
    for i in range(len(tmp_data)):
        varations.append(np.var(streched_data[i]))
        padded_data.append(np.pad(streched_data[i], ((pads_needed[i][0], enough_shape[0]-pads_needed[i][0]-new_shapes[i][0]),(pads_needed[i][1], enough_shape[1]-pads_needed[i][1]-new_shapes[i][1])), 'constant', constant_values=0))
    _X = (np.array(streched_data)+1.0833372e-06)/0.07874487
    __X = np.vstack((np.array(scalers)[:,0], np.array(scalers)[:,1], np.array(speeds))).T
    Y = np.vstack((lengths, widths, depths)).T
    embed()
    for i in range(len(tmp_data)):
        tmp_data[i]['data'] = np.array(PIL.Image.fromarray(padded_data[i]).resize((224,224)))
    #pkl.dump(tmp_data[:int(len(tmp_data)*0.9)], open("/mfs/home/wangke/magnetic-detect/data/data99_train.pkl", 'wb'))
    #pkl.dump(tmp_data[-int(0.1*len(tmp_data)):], open("/mfs/home/wangke/magnetic-detect/data/data99_val.pkl", 'wb'))
    hist_features = []
    steps = 11
    hist_step = np.linspace(-5*np.array(varations).mean(), 5*np.array(varations).mean(), steps)
    for i in range(len(tmp_data)):
        hist_features.append(streched_data[i])
    #mag_data = pd.read_csv("/mfs/home/wangke/magnetic-detect/data/data0.csv")
    embed()
    '''
    #_X = _X.T
    #Y = Y.values.reshape(-1, 1)
    nn_X = torch.autograd.Variable(torch.FloatTensor(_X))
    nn_X2 = torch.autograd.Variable(torch.FloatTensor(__X))
    nn_Y = torch.autograd.Variable(torch.FloatTensor(Y))
    whole_dataset = Data.TensorDataset(nn_X, nn_X2, nn_Y)
    train_dataset, validate_dataset = Data.random_split(whole_dataset, (len(whole_dataset)-int(len(whole_dataset)*0.1),int(len(whole_dataset)*0.1)))
    train_loader = Data.DataLoader( 
            dataset=train_dataset, 
            batch_size=int(len(train_dataset)/args.num_of_batch),
            shuffle=True,
            drop_last=True,
	        num_workers=4,
            pin_memory=True
            )
    validate_loader = Data.DataLoader( 
            dataset=validate_dataset, 
            batch_size=int(len(validate_dataset)/args.num_of_batch),
            shuffle=True,
            drop_last=True,
	        num_workers=4,
            pin_memory=True
            )
    #Model:
    device = torch.device("cuda", args.Cuda_number)
    input_size = nn_X.shape[1]                          
    hidden_size = nn_X.shape[1]*args.hidden_width_scaler
    hidden_depth = args.hidden_depth
    output_size = nn_Y.shape[1]
    #model = NeuralNet(input_size, hidden_size, hidden_depth, output_size, device).to(device)
    model = MagNet(100*100, 3, device).to(device)
    optimizer = optim.SGD(model.parameters(), lr=args.learning_rate)
    #embed()
    #Train and Validate:
    train_loss_history = []
    validate_loss_history = []
    for epoch in range(int(args.max_epoch+1)):
        train_loss = train(args, model, device, train_loader, optimizer, epoch)
        validate_loss, validate_outputs, validate_targets = validate(args, model, device, validate_loader)
        train_loss_history.append(train_loss)
        validate_loss_history.append(validate_loss)
        train_loss_history[0] = validate_loss_history[0]
    NN_friction_predicted = np.array(model.cpu()((nn_X, nn_X2)).detach())
    plt.plot(train_loss_history)
    plt.plot(validate_loss_history)
    plt.show()
    embed()
    #visual(NN_friction_predicted)

    print("Names note:", names_note)
    print("Poly:", opt)
    print("BGD:", params)
    print("NN:", "NONE")
    
